{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Reddit Depression Final Project\n",
        "Link to the paper: https://dl.acm.org/doi/pdf/10.1145/3578503.3583621\n",
        "\n",
        "Read through the paper fully before starting the assignment!"
      ],
      "metadata": {
        "id": "9jFvbbC6VtZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"pandas<2.0.0\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9ygz1X__ysi",
        "outputId": "ff5f26e8-2449-423d-e5f6-34c19550714f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandas<2.0.0\n",
            "  Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.0.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.0.0) (2024.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas<2.0.0) (1.26.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas<2.0.0) (1.17.0)\n",
            "Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.5.3 which is incompatible.\n",
            "mizani 0.13.1 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "plotnine 0.14.3 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "xarray 2024.10.0 requires pandas>=2.1, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-1.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FoBxKQ_OVl-j",
        "outputId": "5895d117-65ff-4b55-b196-4e6a30caca06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import cross_validate, cross_val_score, KFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "FILEPATH = 'drive/MyDrive/student.pkl'\n",
        "processed_control_path = 'drive/MyDrive/control1.pkl'\n",
        "processed_symptom_path = 'drive/MyDrive/symptom1.pkl'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "rcMOTL7mV9T9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of depression subreddits in the paper\n",
        "depression_subreddits = [\"Anger\",\n",
        "    \"anhedonia\", \"DeadBedrooms\",\n",
        "    \"Anxiety\", \"AnxietyDepression\", \"HealthAnxiety\", \"PanicAttack\",\n",
        "    \"DecisionMaking\", \"shouldi\",\n",
        "    \"bingeeating\", \"BingeEatingDisorder\", \"EatingDisorders\", \"eating_disorders\", \"EDAnonymous\",\n",
        "    \"chronicfatigue\", \"Fatigue\",\n",
        "    \"ForeverAlone\", \"lonely\",\n",
        "    \"cry\", \"grief\", \"sad\", \"Sadness\",\n",
        "    \"AvPD\", \"SelfHate\", \"selfhelp\", \"socialanxiety\", \"whatsbotheringyou\",\n",
        "    \"insomnia\", \"sleep\",\n",
        "    \"cfs\", \"ChronicPain\", \"Constipation\", \"EssentialTremor\", \"headaches\", \"ibs\", \"tinnitus\",\n",
        "    \"AdultSelfHarm\", \"selfharm\", \"SuicideWatch\",\n",
        "    \"Guilt\", \"Pessimism\", \"selfhelp\", \"whatsbotheringyou\"\n",
        "]\n",
        "\n",
        "symptom_subreddits = [[\"Anger\"],\n",
        "    [\"anhedonia\", \"DeadBedrooms\"],\n",
        "    [\"Anxiety\", \"AnxietyDepression\", \"HealthAnxiety\", \"PanicAttack\"],\n",
        "    [\"DecisionMaking\", \"shouldi\"],\n",
        "    [\"bingeeating\", \"BingeEatingDisorder\", \"EatingDisorders\", \"eating_disorders\", \"EDAnonymous\"],\n",
        "    [\"chronicfatigue\", \"Fatigue\"],\n",
        "    [\"ForeverAlone\", \"lonely\"],\n",
        "    [\"cry\", \"grief\", \"sad\", \"Sadness\"],\n",
        "    [\"AvPD\", \"SelfHate\", \"selfhelp\", \"socialanxiety\", \"whatsbotheringyou\"],\n",
        "    [\"insomnia\", \"sleep\"],\n",
        "    [\"cfs\", \"ChronicPain\", \"Constipation\", \"EssentialTremor\", \"headaches\", \"ibs\", \"tinnitus\"],\n",
        "    [\"AdultSelfHarm\", \"selfharm\", \"SuicideWatch\"],\n",
        "    [\"Guilt\", \"Pessimism\", \"selfhelp\", \"whatsbotheringyou\"]\n",
        "]"
      ],
      "metadata": {
        "id": "ohOK3wCdWpnA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load(filepath):\n",
        "  \"\"\"Load pickles\"\"\"\n",
        "  with open(filepath, 'rb') as f:\n",
        "      data = pickle.load(f)\n",
        "      return data"
      ],
      "metadata": {
        "id": "icnzto8GWFlb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sym_dataset_generation(raw_data):\n",
        "  \"\"\"Build control and symptom datasets\"\"\"\n",
        "  symptom_dfs = [dict(),dict(),dict(),dict(),dict(),dict(),dict(),dict(),dict(),dict(),dict(),dict(),dict()]\n",
        "  for i in range(13):\n",
        "    symptom_dfs[i] = raw_data[raw_data['subreddit'].isin(symptom_subreddits[i])].copy()\n",
        "    symptom_dfs[i]['created_utc'] = pd.to_datetime(symptom_dfs[i]['created_utc'], unit='s')\n",
        "\n",
        "  return symptom_dfs"
      ],
      "metadata": {
        "id": "1OmDx_IOjIr1"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime, timedelta\n",
        "def dataset_generation(raw_data):\n",
        "  \"\"\"Build control and symptom datasets\"\"\"\n",
        "  symptom_dfs = []\n",
        "  for i in range(13):\n",
        "    symptom_dfs[i] = raw_data[raw_data['subreddit'].isin(symptom_subreddits[i])].copy()\n",
        "    symptom_dfs[i]['created_utc'] = pd.to_datetime(symptom_dfs[i]['created_utc'], unit='s')\n",
        "\n",
        "  control_df = []\n",
        "  authors = symptom_dfs['author'].unique()\n",
        "  authors = set(authors)\n",
        "  print('number of unique:', len(authors))\n",
        "\n",
        "  i=0\n",
        "  for author in authors:\n",
        "      print(i/len(authors)*100)\n",
        "      i+=1\n",
        "      author_symptom_posts = symptom_dfs[symptom_dfs['author'] == author]\n",
        "      earliest_symptom_date = author_symptom_posts['created_utc'].min()\n",
        "\n",
        "      author_posts = raw_data[(raw_data['author'] == author) &\n",
        "                              (~raw_data['subreddit'].isin(depression_subreddits))].copy()\n",
        "      author_posts['created_utc'] = pd.to_datetime(author_posts['created_utc'], unit='s')\n",
        "\n",
        "      valid_control_posts = author_posts[author_posts['created_utc'] <= earliest_symptom_date - timedelta(days=180)]\n",
        "      control_df.append(valid_control_posts)\n",
        "\n",
        "  control_df = pd.concat(control_df, ignore_index=True)\n",
        "  print(f\"Filtered {len(control_df)} control posts.\")\n",
        "\n",
        "  return symptom_dfs, control_df\n"
      ],
      "metadata": {
        "id": "Wpw9kJiras4B"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load raw data from handout\n",
        "raw_data = load(FILEPATH)"
      ],
      "metadata": {
        "id": "84OwmejGe2sB"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#generate data from scratch (obsolete)\n",
        "\n",
        "symptom_dfs, control_df = dataset_generation(raw_data)\n",
        "pickle.dump(symptom_df, open(processed_symptom_path, 'wb'))\n",
        "pickle.dump(control_df, open(processed_control_path, 'wb'))\n",
        "\n",
        "#save symptom dataframes to drive\n",
        "for i in range(13):\n",
        "  symptom_dfs[i].to_pickle(f'drive/MyDrive/symptom{i+1}.pkl')\n",
        "\n",
        "#control too\n",
        "control_df.to_pickle('drive/MyDrive/control1.pkl')"
      ],
      "metadata": {
        "id": "uedZNIYA_PH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#only regenerate symptom data (control takes soo long)\n",
        "symptom_dfs = sym_dataset_generation(raw_data)\n",
        "control_df = load(processed_control_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "zoZArPUICSVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load them back\n",
        "symptom_dfs = []\n",
        "for i in range(13):\n",
        "  symptom_dfs.append(load('drive/MyDrive/symptom{i+1}.pkl'))\n",
        "control_df = load('drive/MyDrive/control1.pkl')"
      ],
      "metadata": {
        "id": "Q80SMpjI8tYD"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reddit Topics with LDA\n",
        "\n",
        " - Don't use MALLET (as the paper does), use some other LDA implementation."
      ],
      "metadata": {
        "id": "U4I37U1SXAEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import LdaMulticore\n",
        "from gensim.corpora import Dictionary\n",
        "import spacy\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from tqdm import tqdm\n"
      ],
      "metadata": {
        "id": "er4OXIirE2fC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We highly recommend you using the LdaMulticore interface, but feel free to use any other implementations if you prefer.\n",
        "\n",
        "from gensim.models import LdaModel\n",
        "from tqdm import tqdm\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\", \"tagger\"])\n",
        "\n",
        "def preprocess_text_spacy(text):\n",
        "    #take out stop words (100 most frequent)\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]\n",
        "    return tokens\n",
        "def preprocess_texts_spacy_batched(texts):\n",
        "    #with tqdm progress bar, batched (faster)\n",
        "    processed_texts = []\n",
        "    for doc in tqdm(nlp.pipe(texts, batch_size=1000), total=len(texts), desc=\"Preprocessing texts\"):\n",
        "        tokens = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]\n",
        "        processed_texts.append(tokens)\n",
        "    return processed_texts\n",
        "\n",
        "def run_lda(data, num_topics=200, passes=1,loadData=False):\n",
        "    # Preprocess text data\n",
        "    tqdm.pandas(desc=\"Processing text\")\n",
        "    if not loadData:\n",
        "      data['processed_text'] = data['text'].progress_apply(preprocess_text_spacy)\n",
        "      data.to_pickle('drive/MyDrive/all_processed_texts.pkl')\n",
        "    else:\n",
        "\n",
        "      processed_texts = load('drive/MyDrive/processed_texts.pkl')\n",
        "      data['processed_text'] = processed_texts\n",
        "\n",
        "    #make dictionary and corpus\n",
        "    print(\"creating dict/corpus\")\n",
        "    dictionary = Dictionary(data['processed_text'])\n",
        "\n",
        "    corpus = [dictionary.doc2bow(text) for text in data['processed_text']]\n",
        "\n",
        "    print(\"Training LDA\")\n",
        "    lda_model = LdaMulticore(corpus, num_topics=num_topics, id2word=dictionary, passes=passes, workers=4)\n",
        "    # lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=passes)\n",
        "\n",
        "    topics = lda_model.print_topics()\n",
        "    for topic in topics:\n",
        "        print(topic)\n",
        "\n",
        "    return lda_model, dictionary, corpus, data\n",
        "\n",
        "# TODO: Your LDA code!"
      ],
      "metadata": {
        "id": "xf3surfWXH-q"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#combine symptom_df and control_df into one flattened df to train LDA\n",
        "\n",
        "all_df = pd.DataFrame(symptom_dfs[0])\n",
        "for i in range(1,13):\n",
        "\n",
        "  all_df = pd.concat([all_df, symptom_dfs[i]])\n",
        "control_df = pd.DataFrame(control_df)\n",
        "\n",
        "all_data = pd.concat([all_df, control_df])\n"
      ],
      "metadata": {
        "id": "TxEOICrOobw6"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_texts = pd.read_csv('drive/MyDrive/symptom_df1.csv')\n",
        "lda_model, dictionary, corpus, proc = run_lda(processed_texts, loadData=False)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "r9esuABSFySz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model, dictionary, corpus, proc = run_lda(processed_texts, loadData=True)\n",
        "\n",
        "\n",
        "X_lda = []\n",
        "y_lda = []\n",
        "for i in range(13):\n",
        "  symptom_corpus = [dictionary.doc2bow(text) for text in processed_sdf[i]['processed_text']]\n",
        "  lda_features = [lda_model.get_document_topics(doc, minimum_probability=0) for doc in symptom_corpus]\n",
        "  lda_array = np.array([[prob for _, prob in doc] for doc in lda_features])\n",
        "  X_lda.append(np.vstack((lda_array, lda_array_c)))\n",
        "  y_lda.append(np.concatenate((np.ones(len(lda_array)), np.zeros(len(lda_array_c)))))\n",
        "\n",
        "for i in range(13):\n",
        "  main(X_lda[i],y_lda[i])"
      ],
      "metadata": {
        "id": "Be5eUTcymTbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RoBERTa Embeddings"
      ],
      "metadata": {
        "id": "E0-97hsVXNkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_roberta_embeddings(data, batch_size=32, layer=5, model_name=\"distilroberta-base\"):\n",
        "\n",
        "    print(\"Loading tokenizer and model...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModel.from_pretrained(model_name)\n",
        "    model.to(\"cuda\")  # Use GPU if available\n",
        "    model.eval()\n",
        "\n",
        "    embeddings = []\n",
        "\n",
        "    print(\"Processing text for embeddings...\")\n",
        "    for i in tqdm(range(0, len(data), batch_size), desc=\"Embedding batches\"):\n",
        "        batch_texts = data['text'][i:i + batch_size].tolist()\n",
        "        inputs = tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs, output_hidden_states=True)\n",
        "            hidden_states = outputs.hidden_states\n",
        "            batch_embeddings = hidden_states[layer].mean(dim=1).cpu().numpy()  # Mean pooling\n",
        "\n",
        "        embeddings.extend(batch_embeddings)\n",
        "\n",
        "    return np.array(embeddings)\n"
      ],
      "metadata": {
        "id": "blx1SWVMXYDp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_features(lda_model, dictionary, corpus, roberta_embeddings):\n",
        "    \"\"\"Combine LDA and RoBERTa embeddings into a single feature set.\"\"\"\n",
        "    print(\"Generating LDA topic distributions...\")\n",
        "    lda_features = [lda_model.get_document_topics(doc, minimum_probability=0) for doc in corpus]\n",
        "    lda_array = np.array([[prob for _, prob in doc] for doc in lda_features])\n",
        "\n",
        "    print(\"Combining features...\")\n",
        "    roberta_embedding = roberta_embeddings.reshape(-1,1)\n",
        "    combined_features = np.hstack((lda_array, roberta_embeddings))\n",
        "    return combined_features\n",
        "\n",
        "def train_and_evaluate_classifier(features, labels):\n",
        "    \"\"\"Train and evaluate a random forest classifier.\"\"\"\n",
        "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "    print(\"Training Random Forest Classifier...\")\n",
        "    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    print(\"Evaluating model...\")\n",
        "    y_pred = clf.predict(X_test)\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    print(\"Cross-validation scores:\")\n",
        "    scores = cross_val_score(clf, features, labels, cv=5)\n",
        "    print(f\"Mean accuracy: {scores.mean():.4f} (+/- {scores.std():.4f})\")\n",
        "\n",
        "    return clf\n"
      ],
      "metadata": {
        "id": "NfVJlLc2fooz"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "symptom_embeddings = []\n",
        "control_embeddings = get_roberta_embeddings(control_df)\n",
        "\n",
        "X_list = []\n",
        "y_list = []\n",
        "for i in range(13):\n",
        "  symptom_embeddings.append(get_roberta_embeddings(symptom_dfs[i]))\n",
        "  symptom_embeddings[i].tofile(f'drive/MyDrive/symptom_embeddings{i+1}.npy')\n",
        "  X_list.append(np.vstack((symptom_embeddings[i], control_embeddings)))\n",
        "  y_list.append(np.concatenate((np.ones(len(symptom_embeddings[i])), np.zeros(len(control_embeddings)))))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WSEzIwQ84aU",
        "outputId": "96df9c07-16b6-429e-98bc-77b342722108"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading tokenizer and model...\n",
            "Processing text for embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Embedding batches: 100%|██████████| 137/137 [00:50<00:00,  2.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading tokenizer and model...\n",
            "Processing text for embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Embedding batches: 100%|██████████| 18/18 [00:08<00:00,  2.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading tokenizer and model...\n",
            "Processing text for embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Embedding batches: 100%|██████████| 186/186 [01:33<00:00,  1.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading tokenizer and model...\n",
            "Processing text for embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Embedding batches: 100%|██████████| 767/767 [05:54<00:00,  2.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading tokenizer and model...\n",
            "Processing text for embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Embedding batches: 100%|██████████| 1/1 [00:00<00:00, 28.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading tokenizer and model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing text for embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Embedding batches: 100%|██████████| 56/56 [00:25<00:00,  2.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading tokenizer and model...\n",
            "Processing text for embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Embedding batches: 100%|██████████| 1/1 [00:00<00:00, 29.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading tokenizer and model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing text for embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Embedding batches: 100%|██████████| 361/361 [02:42<00:00,  2.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading tokenizer and model...\n",
            "Processing text for embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Embedding batches: 100%|██████████| 70/70 [00:31<00:00,  2.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading tokenizer and model...\n",
            "Processing text for embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Embedding batches: 100%|██████████| 309/309 [02:20<00:00,  2.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading tokenizer and model...\n",
            "Processing text for embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Embedding batches: 100%|██████████| 100/100 [00:43<00:00,  2.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading tokenizer and model...\n",
            "Processing text for embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Embedding batches: 100%|██████████| 261/261 [01:57<00:00,  2.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading tokenizer and model...\n",
            "Processing text for embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Embedding batches: 100%|██████████| 829/829 [06:24<00:00,  2.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading tokenizer and model...\n",
            "Processing text for embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Embedding batches: 100%|██████████| 57/57 [00:24<00:00,  2.32it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pickle.dump(X_list, open('drive/MyDrive/Xlist.pkl', 'wb'))\n",
        "pickle.dump(y_list, open('drive/MyDrive/ylist.pkl', 'wb'))\n",
        "\n",
        "for i in range(13):\n",
        "  main(X_list[i],y_list[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CgPbDIyHQ80",
        "outputId": "be4c11df-a4c8-4e5e-e735-8bec5329b53f"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running 5-fold cross-validation...\n",
            "\n",
            "Training Scores:\n",
            "[1. 1. 1. 1. 1.]\n",
            "Testing Scores:\n",
            "[0.93443713 0.94066512 0.94470621 0.94233488 0.95202882]\n",
            "Running 5-fold cross-validation...\n",
            "\n",
            "Training Scores:\n",
            "[1. 1. 1. 1. 1.]\n",
            "Testing Scores:\n",
            "[0.94294323 0.95636956 0.95960694 0.95172144 0.96160825]\n",
            "Running 5-fold cross-validation...\n",
            "\n",
            "Training Scores:\n",
            "[1. 1. 1. 1. 1.]\n",
            "Testing Scores:\n",
            "[0.95685672 0.95019626 0.95500194 0.95619455 0.95430696]\n",
            "Running 5-fold cross-validation...\n",
            "\n",
            "Training Scores:\n",
            "[1. 1. 1. 1. 1.]\n",
            "Testing Scores:\n",
            "[0.4271789  0.84228571 0.62614416 0.39530892 0.93421053]\n",
            "Running 5-fold cross-validation...\n",
            "\n",
            "Training Scores:\n",
            "[1. 1. 1. 1. 1.]\n",
            "Testing Scores:\n",
            "[0.95344863 0.95332038 0.95419402 0.95580584 0.95659453]\n",
            "Running 5-fold cross-validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
            "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py\", line 279, in __call__\n",
            "    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py\", line 371, in _score\n",
            "    y_pred = method_caller(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py\", line 89, in _cached_call\n",
            "    result, _ = _get_response_values(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_response.py\", line 214, in _get_response_values\n",
            "    y_pred = _process_predict_proba(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_response.py\", line 51, in _process_predict_proba\n",
            "    raise ValueError(\n",
            "ValueError: Got predict_proba of shape (874, 1), but need classifier with two classes.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
            "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py\", line 279, in __call__\n",
            "    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py\", line 371, in _score\n",
            "    y_pred = method_caller(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py\", line 89, in _cached_call\n",
            "    result, _ = _get_response_values(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_response.py\", line 214, in _get_response_values\n",
            "    y_pred = _process_predict_proba(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_response.py\", line 51, in _process_predict_proba\n",
            "    raise ValueError(\n",
            "ValueError: Got predict_proba of shape (3496, 1), but need classifier with two classes.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
            "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py\", line 279, in __call__\n",
            "    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py\", line 376, in _score\n",
            "    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py\", line 640, in roc_auc_score\n",
            "    return _average_binary_score(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_base.py\", line 76, in _average_binary_score\n",
            "    return binary_metric(y_true, y_score, sample_weight=sample_weight)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py\", line 382, in _binary_roc_auc_score\n",
            "    raise ValueError(\n",
            "ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
            "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py\", line 279, in __call__\n",
            "    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py\", line 376, in _score\n",
            "    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py\", line 640, in roc_auc_score\n",
            "    return _average_binary_score(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_base.py\", line 76, in _average_binary_score\n",
            "    return binary_metric(y_true, y_score, sample_weight=sample_weight)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py\", line 382, in _binary_roc_auc_score\n",
            "    raise ValueError(\n",
            "ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
            "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py\", line 279, in __call__\n",
            "    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py\", line 376, in _score\n",
            "    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py\", line 640, in roc_auc_score\n",
            "    return _average_binary_score(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_base.py\", line 76, in _average_binary_score\n",
            "    return binary_metric(y_true, y_score, sample_weight=sample_weight)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py\", line 382, in _binary_roc_auc_score\n",
            "    raise ValueError(\n",
            "ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
            "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py\", line 279, in __call__\n",
            "    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py\", line 376, in _score\n",
            "    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py\", line 640, in roc_auc_score\n",
            "    return _average_binary_score(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_base.py\", line 76, in _average_binary_score\n",
            "    return binary_metric(y_true, y_score, sample_weight=sample_weight)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py\", line 382, in _binary_roc_auc_score\n",
            "    raise ValueError(\n",
            "ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.\n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Scores:\n",
            "[nan  1.  1.  1.  1.]\n",
            "Testing Scores:\n",
            "[nan nan nan nan nan]\n",
            "Running 5-fold cross-validation...\n",
            "\n",
            "Training Scores:\n",
            "[0.99999168 0.99998813 1.         0.9999499  0.99998534]\n",
            "Testing Scores:\n",
            "[0.91754488 0.9266538  0.9214281  0.91198491 0.90955192]\n",
            "Running 5-fold cross-validation...\n",
            "\n",
            "Training Scores:\n",
            "[1. 1. 1. 1. 1.]\n",
            "Testing Scores:\n",
            "[0.93503715 0.93163891 0.93369737 0.94000984 0.92986778]\n",
            "Running 5-fold cross-validation...\n",
            "\n",
            "Training Scores:\n",
            "[1. 1. 1. 1. 1.]\n",
            "Testing Scores:\n",
            "[0.93776312 0.92807865 0.94200903 0.93524902 0.92695224]\n",
            "Running 5-fold cross-validation...\n",
            "\n",
            "Training Scores:\n",
            "[1. 1. 1. 1. 1.]\n",
            "Testing Scores:\n",
            "[0.95685038 0.95918502 0.95620205 0.9575227  0.96252877]\n",
            "Running 5-fold cross-validation...\n",
            "\n",
            "Training Scores:\n",
            "[1. 1. 1. 1. 1.]\n",
            "Testing Scores:\n",
            "[0.92874342 0.93433323 0.94067554 0.91880437 0.93839913]\n",
            "Running 5-fold cross-validation...\n",
            "\n",
            "Training Scores:\n",
            "[1. 1. 1. 1. 1.]\n",
            "Testing Scores:\n",
            "[0.96142462 0.96329066 0.96070375 0.96590418 0.96936106]\n",
            "Running 5-fold cross-validation...\n",
            "\n",
            "Training Scores:\n",
            "[1. 1. 1. 1. 1.]\n",
            "Testing Scores:\n",
            "[0.9301596  0.92209714 0.92970534 0.93222255 0.91485885]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main"
      ],
      "metadata": {
        "id": "rDWxuF2jXtwi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2iZdUntPI6LT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "def main(X, y):\n",
        "    \"\"\"\n",
        "    Here's the basic structure of the main block! It should run\n",
        "    5-fold cross validation with random forest to evaluate your RoBERTa and LDA\n",
        "    performance.\n",
        "    \"\"\"\n",
        "    rf_classifier = RandomForestClassifier(verbose=1)\n",
        "    cv = KFold(n_splits=5, shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "    print(\"Running 5-fold cross-validation...\")\n",
        "    rf_classifier = RandomForestClassifier()\n",
        "    cv = KFold(n_splits=5, shuffle=True)\n",
        "    results = cross_validate(rf_classifier, X=X, y=y, cv=cv, scoring='roc_auc', return_train_score=True)\n",
        "\n",
        "    print(\"\\nTraining Scores:\")\n",
        "    print(results.get('train_score'))\n",
        "    print(\"Testing Scores:\")\n",
        "    print(results.get('test_score'))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "koTBPhcDXujb"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model, dictionary, corpus = run_lda(all_data,loadData=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FItPyG49BQ0u",
        "outputId": "f9da4b3d-b01e-43e1-db49-e8163dd5014c"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing text...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing text:   0%|          | 0/100633 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n",
            "Processing text: 100%|██████████| 100633/100633 [29:56<00:00, 56.02it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating dictionary and corpus...\n",
            "Training LDA model...\n",
            "(56, '0.029*\"friend\" + 0.019*\"dreams\" + 0.017*\"anymore\" + 0.015*\"friends\" + 0.013*\"time\" + 0.013*\"know\" + 0.012*\"like\" + 0.012*\"best\" + 0.011*\"life\" + 0.009*\"going\"')\n",
            "(77, '0.016*\"like\" + 0.014*\"time\" + 0.010*\"years\" + 0.008*\"want\" + 0.007*\"feel\" + 0.007*\"life\" + 0.006*\"artist\" + 0.006*\"going\" + 0.006*\"know\" + 0.006*\"relationship\"')\n",
            "(198, '0.014*\"anxiety\" + 0.011*\"know\" + 0.011*\"like\" + 0.010*\"series\" + 0.010*\"razors\" + 0.009*\"tired\" + 0.008*\"feel\" + 0.007*\"sleepy\" + 0.007*\"adderall\" + 0.006*\"aids\"')\n",
            "(49, '0.016*\"audio\" + 0.012*\"like\" + 0.011*\"time\" + 0.009*\"kratom\" + 0.007*\"want\" + 0.007*\"know\" + 0.007*\"work\" + 0.007*\"going\" + 0.007*\"feel\" + 0.007*\"help\"')\n",
            "(91, '0.021*\"life\" + 0.013*\"patients\" + 0.013*\"like\" + 0.012*\"know\" + 0.010*\"people\" + 0.008*\"want\" + 0.008*\"self\" + 0.006*\"way\" + 0.006*\"think\" + 0.005*\"things\"')\n",
            "(168, '0.024*\"suicidal\" + 0.019*\"going\" + 0.019*\"like\" + 0.014*\"feel\" + 0.012*\"want\" + 0.012*\"thoughts\" + 0.011*\"life\" + 0.011*\"die\" + 0.010*\"time\" + 0.009*\"know\"')\n",
            "(17, '0.019*\"snoring\" + 0.011*\"email\" + 0.011*\"fans\" + 0.010*\"banned\" + 0.010*\"like\" + 0.009*\"know\" + 0.009*\"parasite\" + 0.008*\"tube\" + 0.007*\"feel\" + 0.007*\"time\"')\n",
            "(113, '0.013*\"like\" + 0.013*\"facial\" + 0.009*\"want\" + 0.009*\"feel\" + 0.009*\"years\" + 0.008*\"politics\" + 0.008*\"know\" + 0.007*\"political\" + 0.007*\"time\" + 0.007*\"expression\"')\n",
            "(60, '0.017*\"like\" + 0.015*\"know\" + 0.015*\"feel\" + 0.012*\"sex\" + 0.011*\"people\" + 0.011*\"time\" + 0.008*\"want\" + 0.007*\"years\" + 0.007*\"things\" + 0.006*\"life\"')\n",
            "(134, '0.015*\"noise\" + 0.011*\"loud\" + 0.009*\"anxiety\" + 0.009*\"know\" + 0.009*\"hear\" + 0.009*\"tea\" + 0.008*\"hearing\" + 0.008*\"survived\" + 0.008*\"like\" + 0.008*\"headphones\"')\n",
            "(157, '0.050*\"people\" + 0.020*\"like\" + 0.020*\"want\" + 0.017*\"know\" + 0.011*\"friends\" + 0.010*\"time\" + 0.010*\"think\" + 0.009*\"life\" + 0.009*\"things\" + 0.007*\"hate\"')\n",
            "(115, '0.027*\"harming\" + 0.020*\"rope\" + 0.019*\"feel\" + 0.019*\"speech\" + 0.016*\"like\" + 0.012*\"self\" + 0.010*\"want\" + 0.008*\"know\" + 0.007*\"time\" + 0.007*\"started\"')\n",
            "(40, '0.135*\"nt\" + 0.067*\"m\" + 0.018*\"like\" + 0.016*\"know\" + 0.016*\"ve\" + 0.015*\"feel\" + 0.013*\"want\" + 0.012*\"s\" + 0.009*\"anymore\" + 0.007*\"life\"')\n",
            "(102, '0.039*\"scars\" + 0.015*\"humans\" + 0.013*\"feel\" + 0.013*\"life\" + 0.011*\"time\" + 0.011*\"like\" + 0.010*\"want\" + 0.010*\"think\" + 0.009*\"know\" + 0.008*\"people\"')\n",
            "(57, '0.022*\"like\" + 0.017*\"know\" + 0.016*\"anxiety\" + 0.016*\"feel\" + 0.011*\"friends\" + 0.011*\"college\" + 0.011*\"time\" + 0.009*\"want\" + 0.008*\"life\" + 0.007*\"think\"')\n",
            "(142, '0.028*\"job\" + 0.026*\"work\" + 0.016*\"like\" + 0.015*\"feel\" + 0.011*\"time\" + 0.011*\"life\" + 0.010*\"want\" + 0.009*\"know\" + 0.008*\"working\" + 0.005*\"years\"')\n",
            "(96, '0.016*\"rich\" + 0.012*\"need\" + 0.011*\"help\" + 0.010*\"faster\" + 0.010*\"work\" + 0.010*\"contemplating\" + 0.008*\"pm\" + 0.008*\"crush\" + 0.008*\"know\" + 0.007*\"like\"')\n",
            "(197, '0.022*\"blade\" + 0.017*\"poop\" + 0.013*\"driving\" + 0.011*\"feel\" + 0.011*\"car\" + 0.010*\"like\" + 0.009*\"want\" + 0.009*\"know\" + 0.008*\"overdosing\" + 0.008*\"cars\"')\n",
            "(138, '0.016*\"self\" + 0.011*\"life\" + 0.011*\"depression\" + 0.009*\"know\" + 0.009*\"time\" + 0.009*\"help\" + 0.009*\"like\" + 0.008*\"anxiety\" + 0.008*\"feel\" + 0.006*\"things\"')\n",
            "(151, '0.015*\"rid\" + 0.012*\"military\" + 0.011*\"pain\" + 0.010*\"like\" + 0.009*\"disappointment\" + 0.008*\"states\" + 0.008*\"bright\" + 0.007*\"humanity\" + 0.007*\"af\" + 0.007*\"know\"')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model.save('drive/MyDrive/lda_model.model')\n"
      ],
      "metadata": {
        "id": "AKMKyUij6Sph"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load lda model\n",
        "lda_model = LdaMulticore.load('drive/MyDrive/lda_model.model')"
      ],
      "metadata": {
        "id": "JQLB6Ean6UNJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-PB9_Ef9yUH",
        "outputId": "dc611067-e90d-45d4-c429-8e425fee5622"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5338       Advice on dealing with anger? Normally I'm a c...\n",
            "5594       I've been to anger management 10 times all it'...\n",
            "11246                         Ripping heads off :) [removed]\n",
            "13284      Things that piss me off most. Being lonely. \\n...\n",
            "20424                              Weird black guy [removed]\n",
            "                                 ...                        \n",
            "1955075    How To Stop This Before it Gets Out Of Hand? H...\n",
            "1958806           Hypocrisy has never been so real [removed]\n",
            "1960247    I hate myself after losing control First time ...\n",
            "1960734    I’m not a particularly angry person, but today...\n",
            "1964193    Sometimes I can literally feel my neurology sh...\n",
            "Name: text, Length: 555, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "symptom_embeddings.tofile('drive/MyDrive/symptom_embeddings.npy')\n",
        "control_embeddings.tofile('drive/MyDrive/control_embeddings.npy')"
      ],
      "metadata": {
        "id": "tTlx9972fsY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "symptom_embeddings = np.fromfile('drive/MyDrive/symptom_embeddings.npy')\n",
        "control_embeddings = np.fromfile('drive/MyDrive/control_embeddings.npy')"
      ],
      "metadata": {
        "id": "C29rEHqQMpuM"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model_symptom, dictionary_symptom, corpus_symptom = run_lda(symptom_df, loadData=True)  # LDA for symptom data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSwy8WChk1uy",
        "outputId": "d2aed665-0ceb-4931-a3c1-65a4e6d89431"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing text...\n",
            "Creating dictionary and corpus...\n",
            "Training LDA model...\n",
            "(38, '0.016*\"weight\" + 0.015*\"like\" + 0.013*\"anxiety\" + 0.011*\"feel\" + 0.010*\"eat\" + 0.010*\"know\" + 0.009*\"felt\" + 0.008*\"time\" + 0.008*\"day\" + 0.008*\"appetite\"')\n",
            "(11, '0.031*\"feel\" + 0.021*\"like\" + 0.020*\"know\" + 0.017*\"want\" + 0.011*\"hug\" + 0.009*\"day\" + 0.009*\"going\" + 0.008*\"life\" + 0.007*\"help\" + 0.007*\"better\"')\n",
            "(79, '0.014*\"headphones\" + 0.009*\"pain\" + 0.008*\"want\" + 0.007*\"like\" + 0.007*\"going\" + 0.007*\"day\" + 0.007*\"injections\" + 0.007*\"shot\" + 0.007*\"recommendation\" + 0.006*\"pool\"')\n",
            "(138, '0.027*\"like\" + 0.025*\"feel\" + 0.012*\"anxiety\" + 0.012*\"know\" + 0.010*\"want\" + 0.009*\"life\" + 0.008*\"time\" + 0.007*\"drawn\" + 0.006*\"going\" + 0.006*\"people\"')\n",
            "(150, '0.027*\"birthday\" + 0.015*\"like\" + 0.013*\"know\" + 0.012*\"said\" + 0.011*\"feel\" + 0.009*\"got\" + 0.008*\"time\" + 0.008*\"day\" + 0.008*\"going\" + 0.007*\"told\"')\n",
            "(156, '0.018*\"feel\" + 0.017*\"want\" + 0.017*\"like\" + 0.014*\"m\" + 0.013*\"know\" + 0.011*\"people\" + 0.011*\"nt\" + 0.011*\"life\" + 0.008*\"bad\" + 0.007*\"think\"')\n",
            "(133, '0.029*\"cut\" + 0.029*\"harm\" + 0.028*\"self\" + 0.017*\"help\" + 0.013*\"knife\" + 0.013*\"picture\" + 0.012*\"relapse\" + 0.009*\"proud\" + 0.009*\"trying\" + 0.009*\"coping\"')\n",
            "(140, '0.044*\"want\" + 0.019*\"die\" + 0.016*\"anymore\" + 0.016*\"know\" + 0.012*\"like\" + 0.010*\"day\" + 0.010*\"time\" + 0.010*\"feel\" + 0.008*\"going\" + 0.008*\"friends\"')\n",
            "(18, '0.089*\"nt\" + 0.047*\"m\" + 0.021*\"know\" + 0.019*\"like\" + 0.019*\"s\" + 0.018*\"life\" + 0.017*\"want\" + 0.013*\"feel\" + 0.012*\"anymore\" + 0.011*\"people\"')\n",
            "(169, '0.020*\"like\" + 0.012*\"awkward\" + 0.012*\"event\" + 0.010*\"feel\" + 0.010*\"people\" + 0.010*\"know\" + 0.008*\"girl\" + 0.008*\"talk\" + 0.007*\"think\" + 0.007*\"day\"')\n",
            "(24, '0.035*\"eye\" + 0.027*\"people\" + 0.025*\"like\" + 0.021*\"blah\" + 0.016*\"blank\" + 0.016*\"look\" + 0.015*\"o\" + 0.013*\"y\" + 0.012*\"face\" + 0.012*\"person\"')\n",
            "(132, '0.026*\"like\" + 0.018*\"feel\" + 0.015*\"know\" + 0.010*\"life\" + 0.009*\"things\" + 0.009*\"going\" + 0.008*\"time\" + 0.008*\"think\" + 0.008*\"anxiety\" + 0.007*\"people\"')\n",
            "(70, '0.051*\"want\" + 0.025*\"like\" + 0.023*\"feel\" + 0.022*\"know\" + 0.020*\"life\" + 0.015*\"people\" + 0.010*\"hate\" + 0.009*\"think\" + 0.008*\"die\" + 0.008*\"friends\"')\n",
            "(40, '0.028*\"test\" + 0.013*\"anxiety\" + 0.011*\"help\" + 0.008*\"getting\" + 0.007*\"diet\" + 0.007*\"pregnant\" + 0.007*\"low\" + 0.007*\"months\" + 0.006*\"need\" + 0.006*\"year\"')\n",
            "(108, '0.015*\"like\" + 0.015*\"feel\" + 0.014*\"friends\" + 0.011*\"people\" + 0.011*\"want\" + 0.011*\"anxiety\" + 0.011*\"help\" + 0.011*\"know\" + 0.009*\"time\" + 0.008*\"therapy\"')\n",
            "(162, '0.042*\"help\" + 0.018*\"therapy\" + 0.013*\"cbt\" + 0.011*\"hotline\" + 0.011*\"know\" + 0.009*\"treatment\" + 0.008*\"need\" + 0.008*\"cure\" + 0.008*\"clinical\" + 0.007*\"want\"')\n",
            "(87, '0.015*\"time\" + 0.013*\"want\" + 0.012*\"life\" + 0.010*\"know\" + 0.010*\"friends\" + 0.010*\"money\" + 0.008*\"work\" + 0.008*\"got\" + 0.008*\"going\" + 0.007*\"like\"')\n",
            "(197, '0.043*\"wanna\" + 0.022*\"everybody\" + 0.020*\"like\" + 0.016*\"feel\" + 0.012*\"subreddit\" + 0.011*\"want\" + 0.011*\"discord\" + 0.010*\"know\" + 0.009*\"die\" + 0.008*\"love\"')\n",
            "(107, '0.038*\"eating\" + 0.038*\"food\" + 0.037*\"eat\" + 0.021*\"binge\" + 0.013*\"know\" + 0.012*\"cut\" + 0.011*\"days\" + 0.010*\"like\" + 0.009*\"foods\" + 0.008*\"day\"')\n",
            "(166, '0.016*\"cancer\" + 0.013*\"think\" + 0.012*\"know\" + 0.012*\"anxiety\" + 0.009*\"feel\" + 0.009*\"time\" + 0.007*\"help\" + 0.007*\"like\" + 0.007*\"day\" + 0.006*\"years\"')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model_control, dictionary_control, corpus_control = run_lda(control_df, loadData=False)  # LDA for control data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IurQ_3K5gVjv",
        "outputId": "d3adbb19-394b-4da2-a22b-73694b08ade8"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing text...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing text:   0%|          | 0/4369 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n",
            "Processing text: 100%|██████████| 4369/4369 [00:34<00:00, 127.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating dictionary and corpus...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training LDA model...\n",
            "(82, '0.019*\"think\" + 0.013*\"tiger\" + 0.013*\"tooth\" + 0.013*\"black\" + 0.013*\"karambit\" + 0.013*\"keys\" + 0.013*\"games\" + 0.013*\"going\" + 0.012*\"okay\" + 0.011*\"time\"')\n",
            "(125, '0.022*\"like\" + 0.019*\"game\" + 0.014*\"know\" + 0.014*\"season\" + 0.013*\"think\" + 0.012*\"got\" + 0.011*\"saying\" + 0.011*\"goes\" + 0.010*\"people\" + 0.010*\"time\"')\n",
            "(67, '0.020*\"like\" + 0.013*\"feel\" + 0.013*\"want\" + 0.011*\"going\" + 0.010*\"internet\" + 0.010*\"asked\" + 0.010*\"nt\" + 0.009*\"play\" + 0.009*\"water\" + 0.009*\"wanted\"')\n",
            "(3, '0.041*\"like\" + 0.026*\"people\" + 0.018*\"want\" + 0.015*\"know\" + 0.012*\"bad\" + 0.011*\"find\" + 0.010*\"life\" + 0.010*\"sign\" + 0.009*\"day\" + 0.008*\"going\"')\n",
            "(62, '0.015*\"going\" + 0.013*\"year\" + 0.011*\"told\" + 0.011*\"old\" + 0.010*\"time\" + 0.010*\"dream\" + 0.009*\"friends\" + 0.009*\"wo\" + 0.009*\"random\" + 0.008*\"floor\"')\n",
            "(55, '0.016*\"like\" + 0.011*\"know\" + 0.010*\"s\" + 0.010*\"nt\" + 0.009*\"got\" + 0.008*\"point\" + 0.008*\"body\" + 0.007*\"shit\" + 0.007*\"pc\" + 0.007*\"time\"')\n",
            "(9, '0.014*\"people\" + 0.014*\"want\" + 0.013*\"things\" + 0.011*\"like\" + 0.011*\"nt\" + 0.009*\"pc\" + 0.009*\"game\" + 0.009*\"got\" + 0.008*\"away\" + 0.008*\"sure\"')\n",
            "(131, '0.018*\"like\" + 0.014*\"game\" + 0.013*\"people\" + 0.013*\"server\" + 0.011*\"time\" + 0.008*\"want\" + 0.007*\"long\" + 0.007*\"pretty\" + 0.007*\"going\" + 0.007*\"lot\"')\n",
            "(107, '0.032*\"like\" + 0.019*\"need\" + 0.018*\"heart\" + 0.017*\"got\" + 0.016*\"want\" + 0.012*\"feel\" + 0.011*\"going\" + 0.011*\"sex\" + 0.009*\"away\" + 0.009*\"felt\"')\n",
            "(64, '0.019*\"know\" + 0.018*\"california\" + 0.018*\"like\" + 0.013*\"games\" + 0.013*\"found\" + 0.013*\"people\" + 0.013*\"better\" + 0.011*\"think\" + 0.009*\"feel\" + 0.009*\"days\"')\n",
            "(190, '0.020*\"server\" + 0.019*\"need\" + 0.013*\"tooth\" + 0.013*\"item\" + 0.012*\"keys\" + 0.012*\"tiger\" + 0.012*\"map\" + 0.012*\"art\" + 0.012*\"phone\" + 0.010*\"concerns\"')\n",
            "(102, '0.015*\"feel\" + 0.013*\"like\" + 0.012*\"want\" + 0.011*\"medication\" + 0.011*\"keys\" + 0.010*\"relationship\" + 0.009*\"house\" + 0.009*\"little\" + 0.009*\"work\" + 0.009*\"know\"')\n",
            "(15, '0.019*\"happened\" + 0.016*\"like\" + 0.016*\"left\" + 0.016*\"weeks\" + 0.014*\"thing\" + 0.013*\"came\" + 0.013*\"lot\" + 0.013*\"months\" + 0.013*\"trying\" + 0.012*\"night\"')\n",
            "(187, '0.024*\"like\" + 0.024*\"feel\" + 0.019*\"use\" + 0.013*\"ride\" + 0.012*\"want\" + 0.012*\"way\" + 0.011*\"speed\" + 0.010*\"second\" + 0.010*\"fun\" + 0.009*\"said\"')\n",
            "(112, '0.017*\"time\" + 0.014*\"sex\" + 0.013*\"got\" + 0.013*\"want\" + 0.012*\"server\" + 0.012*\"know\" + 0.010*\"character\" + 0.009*\"children\" + 0.008*\"battery\" + 0.008*\"progress\"')\n",
            "(144, '0.018*\"like\" + 0.017*\"trump\" + 0.014*\"today\" + 0.013*\"things\" + 0.011*\"good\" + 0.010*\"help\" + 0.010*\"getting\" + 0.010*\"time\" + 0.010*\"rules\" + 0.009*\"know\"')\n",
            "(89, '0.017*\"want\" + 0.014*\"amp\" + 0.014*\"hours\" + 0.013*\"time\" + 0.012*\"feel\" + 0.012*\"family\" + 0.011*\"drive\" + 0.011*\"like\" + 0.011*\"love\" + 0.011*\"know\"')\n",
            "(46, '0.017*\"like\" + 0.012*\"day\" + 0.011*\"getting\" + 0.010*\"want\" + 0.009*\"help\" + 0.009*\"surgery\" + 0.009*\"sets\" + 0.008*\"know\" + 0.008*\"trying\" + 0.008*\"told\"')\n",
            "(26, '0.019*\"work\" + 0.017*\"way\" + 0.017*\"need\" + 0.015*\"use\" + 0.015*\"studying\" + 0.015*\"continue\" + 0.014*\"time\" + 0.014*\"card\" + 0.014*\"copy\" + 0.014*\"program\"')\n",
            "(34, '0.024*\"feel\" + 0.018*\"think\" + 0.017*\"time\" + 0.016*\"contact\" + 0.014*\"experience\" + 0.012*\"told\" + 0.012*\"know\" + 0.012*\"day\" + 0.011*\"way\" + 0.011*\"account\"')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lda_features = [lda_model_symptom.get_document_topics(doc, minimum_probability=0) for doc in corpus_symptom]\n",
        "lda_array = np.array([[prob for _, prob in doc] for doc in lda_features])"
      ],
      "metadata": {
        "id": "dRApUtrIxEXB"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sym_embedding_re = symptom_embeddings.reshape(lda_array.shape[0], -1)\n",
        "print(sym_embedding_re.shape)\n",
        "print(lda_array.shape)\n",
        "combined_features_sym = np.hstack((lda_array, sym_embedding_re))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "wVSDPHwQ5BHq",
        "outputId": "a966daa3-2b29-40a7-bfc8-00b4ce8a3906"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'symptom_embeddings' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-c09153ac7519>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msym_embedding_re\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msymptom_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msym_embedding_re\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcombined_features_sym\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msym_embedding_re\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'symptom_embeddings' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lda_features_c = [lda_model_control.get_document_topics(doc, minimum_probability=0) for doc in corpus_control]\n",
        "lda_array_c = np.array([[prob for _, prob in doc] for doc in lda_features_c])\n",
        "\n",
        "ctrl_embedding_re = control_embeddings.reshape(lda_array_c.shape[0], -1)\n",
        "\n",
        "combined_features_c = np.hstack((lda_array_c, ctrl_embedding_re))"
      ],
      "metadata": {
        "id": "S1W5VRoR6aAd"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.vstack((combined_features_sym, combined_features_c))\n",
        "y = np.concatenate((np.ones(len(combined_features_sym)), np.zeros(len(combined_features_c))))\n",
        "X.tofile('drive/MyDrive/X.npy')\n",
        "y.tofile('drive/MyDrive/y.npy')"
      ],
      "metadata": {
        "id": "iDwGMum2HhuI"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.fromfile('drive/MyDrive/X.npy')\n",
        "y = np.fromfile('drive/MyDrive/y.npy')\n",
        "main(X,y)"
      ],
      "metadata": {
        "id": "WkKpQmsgH1cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "si = 0\n",
        "lda_features = [lda_model.get_document_topics(doc, minimum_probability=0) for doc in corpus]\n",
        "lda_array = np.array([[prob for _, prob in doc] for doc in lda_features])\n",
        "sym_embedding_re = symptom_embeddings.reshape(lda_array.shape[0], -1)\n",
        "print(sym_embedding_re.shape)\n",
        "print(lda_array.shape)\n",
        "combined_features_sym = np.hstack((lda_array, sym_embedding_re))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhM7OqxzWbTE",
        "outputId": "fe2dfc6a-203e-441a-85da-dbf4952892eb"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(98883, 584)\n",
            "(98883,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.vstack((combined_features_sym, combined_features_c))\n",
        "y = np.concatenate((np.ones(len(combined_features_sym)), np.zeros(len(combined_features_c))))\n",
        "main(X,y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "rxgubK2c7Dgf",
        "outputId": "7e464e17-fcd3-444f-987f-cab15c97cbbe"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running 5-fold cross-validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rCross-validation folds:   0%|          | 0/5 [00:00<?, ?it/s][Parallel(n_jobs=1)]: Done  49 tasks      | elapsed: 11.0min\n",
            "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    2.1s\n",
            "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.5s\n",
            "Cross-validation folds:  20%|██        | 1/5 [32:44<2:10:57, 1964.39s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-a9e769e08ee5>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_features_sym\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcombined_features_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_features_sym\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_features_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-34-20419f12c897>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mrf_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Evaluate on training and testing sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1471\u001b[0m                 )\n\u001b[1;32m   1472\u001b[0m             ):\n\u001b[0;32m-> 1473\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m             \u001b[0;31m# since correctness does not rely on using threads.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             trees = Parallel(\n\u001b[0m\u001b[1;32m    490\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         )\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"balanced\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         tree._fit(\n\u001b[0m\u001b[1;32m    193\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    470\u001b[0m             )\n\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing_values_in_feature_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "symptom_features = combine_features(lda_model_symptom, dictionary_symptom, corpus_symptom, symptom_embeddings)\n",
        "control_features = combine_features(lda_model_control, dictionary_control, corpus_control, control_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "QEGA4Pm8lONq",
        "outputId": "b400d2f3-04df-4117-a787-8359cbf5fcd2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'corpus' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-cc56579a8a0b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlda_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlda_model_symptom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_document_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminimum_probability\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlda_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprob\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlda_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msymptom_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombine_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_model_symptom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary_symptom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_symptom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymptom_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcontrol_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombine_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_model_control\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary_control\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_control\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'corpus' is not defined"
          ]
        }
      ]
    }
  ]
}